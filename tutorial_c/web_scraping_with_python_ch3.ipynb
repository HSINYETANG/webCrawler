{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ch3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "html = urlopen(\"https://en.wikipedia.org/wiki/Kevin_Bacon\")\n",
    "bsObj = BeautifulSoup(html.read(), \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for link in bsObj.findAll(\"a\"):\n",
    "    if \"href\" in link.attrs:\n",
    "        print(link.attrs[\"href\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for link in bsObj.find(\"div\", {\"id\": \"bodyContent\"}).findAll(\"a\", href = re.compile(\"^(/wiki/)((?!:).)*$\")):\n",
    "    if \"href\" in link.attrs:\n",
    "        print(link.attrs[\"href\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.seed(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getLinks(articleUrl):\n",
    "    html = urlopen(\"https://en.wikipedia.org\"+articleUrl)\n",
    "    bsObj = BeautifulSoup(html, \"lxml\")\n",
    "    return bsObj.find(\"div\", {\"id\": \"bodyContent\"}).findAll(\"a\", href = re.compile(\"^(/wiki/)((?!:).)*$\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "links = getLinks(\"/wiki/Kevin_Bacon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "while len(links)>0:\n",
    "    newArticle = links[random.randint(0, len(links)-1)].attrs[\"href\"]\n",
    "    print(newArticle)\n",
    "    links = getLinks(newArticle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3.2 採集整個網站"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 ex1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pages = set()\n",
    "def getLinks(pageUrl):\n",
    "    global pages\n",
    "    html = urlopen(\"https://en.wikipedia.org\"+pageUrl)\n",
    "    bsObj = BeautifulSoup(html, \"lxml\")\n",
    "    for link in bsObj.findAll(\"a\", href = re.compile(\"^(/wiki/)\")):\n",
    "        if(\"href\" in link.attrs):\n",
    "            if(link.attrs[\"href\"] not in pages):\n",
    "                #Face new page\n",
    "                newPage = link.attrs[\"href\"]\n",
    "                print(newPage)\n",
    "                pages.add(newPage)\n",
    "                getLinks(newPage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "getLinks(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 ex2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pages = set()\n",
    "def getLinks(pageUrl):\n",
    "    global pages\n",
    "    html = urlopen(\"https://en.wikipedia.org\"+pageUrl)\n",
    "    bsObj = BeautifulSoup(html, \"lxml\")\n",
    "    try:\n",
    "        print(bsObj.h1.get_text())\n",
    "        print(bsObj.find(id = \"mw-content-text\").findAll(\"p\")[0])\n",
    "        print(bsObj.find(id = \"ca-edit\").find(\"span\").find(\"a\").attrs[\"href\"])\n",
    "    except AttributeError:\n",
    "        print(\"頁面缺少一些屬性，不過不用擔心\")\n",
    "    for link in bsObj.findAll(\"a\", href = re.compile(\"^(/wiki/)\")):\n",
    "        if(\"href\" in link.attrs):\n",
    "            if(link.attrs[\"href\"] not in pages):\n",
    "                #Face new page\n",
    "                newPage = link.attrs[\"href\"]\n",
    "                print(\"--------------------\\n\"+newPage)\n",
    "                pages.add(newPage)\n",
    "                getLinks(newPage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "getLinks(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3 通過互聯網採集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ex1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import datetime\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pages = set()\n",
    "random.seed(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getInternalLinks(bsObj, includeUrl):\n",
    "    #獲取頁面所有的內鏈列表\n",
    "    internalLinks = []\n",
    "    #找出所有以\"/\"開頭的鏈結\n",
    "    for link in bsObj.findAll(\"a\", href = re.compile(\"^(/|.*\"+includeUrl+\")\")):\n",
    "        if link.attrs[\"href\"] is not None:\n",
    "            if( link.attrs[\"href\"] not in internalLinks):\n",
    "                internalLinks.append(link.attrs[\"href\"])\n",
    "    return internalLinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "startingPage = \"http://oreilly.com\"\n",
    "html = urlopen(startingPage)\n",
    "bsObj = BeautifulSoup(html, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getExternalLinks(bsObj, excludeUrl):\n",
    "    externalLinks = []\n",
    "    #找出所有以\"http\"或\"www\"開頭且不包含當前URL的鏈結\n",
    "    for link in bsObj.findAll(\"a\", href = re.compile(\"^(http|www)((?!\"+excludeUrl+\").)*$\")):\n",
    "        if link.attrs[\"href\"] is not None:\n",
    "            if( link.attrs[\"href\"] not in externalLinks):\n",
    "                externalLinks.append(link.attrs[\"href\"])\n",
    "    return externalLinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def splitAddress(address):\n",
    "    addressParts = address.replace(\"http://\", \"\").split(\"/\")\n",
    "    return addressParts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getRandomExternalLink(startingPage):\n",
    "    html = urlopen(startingPage)\n",
    "    bsObj = BeautifulSoup(html, \"lxml\")\n",
    "    externalLinks = getExternalLinks(bsObj, splitAddress(startingPage)[0])\n",
    "    if( len(externalLinks)==0):\n",
    "        internalLinks = getInternalLinks(startingPage)\n",
    "        return getNextExternalLink( internalLinks[random.randint(0, len(internalLinks)-1)])\n",
    "    else:\n",
    "        return externalLinks[random.randint(0, len(externalLinks)-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def followExternalOnly(startingSite):\n",
    "    externalLink = getRandomExternalLink(\"http://oreilly.com\")\n",
    "    print(\"隨機外鏈是:\"+externalLink)\n",
    "    followExternalOnly(startingSite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "followExternalOnly(\"http://oreilly.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import datetime\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#收集網站上發現的所有外鏈列表\n",
    "allExtLinks = set()\n",
    "allIntLinks = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getInternalLinks(bsObj, includeUrl):\n",
    "    #獲取頁面所有的內鏈列表\n",
    "    internalLinks = []\n",
    "    #找出所有以\"/\"開頭的鏈結\n",
    "    for link in bsObj.findAll(\"a\", href = re.compile(\"^(/|.*\"+includeUrl+\")\")):\n",
    "        if link.attrs[\"href\"] is not None:\n",
    "            if( link.attrs[\"href\"] not in internalLinks):\n",
    "                internalLinks.append(link.attrs[\"href\"])\n",
    "    return internalLinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getExternalLinks(bsObj, excludeUrl):\n",
    "    externalLinks = []\n",
    "    #找出所有以\"http\"或\"www\"開頭且不包含當前URL的鏈結\n",
    "    for link in bsObj.findAll(\"a\", href = re.compile(\"^(http|www)((?!\"+excludeUrl+\").)*$\")):\n",
    "        if link.attrs[\"href\"] is not None:\n",
    "            if( link.attrs[\"href\"] not in externalLinks):\n",
    "                externalLinks.append(link.attrs[\"href\"])\n",
    "    return externalLinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def splitAddress(address):\n",
    "    addressParts = address.replace(\"http://\", \"\").split(\"/\")\n",
    "    return addressParts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAllExternalLinks(siteUrl):\n",
    "    html = urlopen(siteUrl)\n",
    "    bsObj = BeautifulSoup(html, \"lxml\")\n",
    "    internalLinks = getInternalLinks(bsObj, splitAddress(siteUrl)[0])\n",
    "    externalLinks = getExternalLinks(bsObj, splitAddress(siteUrl)[0])\n",
    "    for link in externalLinks:\n",
    "        if link not in allExtLinks:\n",
    "            allExtLinks.add(link)\n",
    "            print(link)\n",
    "    for link in internalLinks:\n",
    "        if link not in allIntLinks:\n",
    "            allIntLinks.add(link)\n",
    "            print(link)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "getAllExternalLinks(\"http://oreilly.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4用scrpay採集 (參考官網的scrpay教學)\n",
    "#### 英文教學https://doc.scrapy.org/en/latest/intro/tutorial.html\n",
    "#### 中文教學: http://scrapy-chs.readthedocs.io/zh_CN/latest/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://doc.scrapy.org/en/latest/intro/tutorial.html\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
